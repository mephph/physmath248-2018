{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data modeling\n",
    "\n",
    "## Moments of distribution\n",
    "\n",
    "**Literature:** Press etal. _Numerical Recipies_, Chapter 14.1\n",
    "\n",
    "Moments are the sums of integer powers of data values $(x_1, x_2,\n",
    "x_3, \\dots, x_\\mathrm{N})$. They characterize the distribution of\n",
    "data values if the data shows a sufficiently strong tendency to \n",
    "cluster around some particular value.\n",
    "\n",
    "### Mean\n",
    "\n",
    "$1^{st}$ moment:\n",
    "\n",
    "$$  <x> = \\bar{x} = \\frac{1}{N} \\sum_{j=1}^N x_j  $$\n",
    "\n",
    "### Variance/width\n",
    "\n",
    "$2^{nd}$ moment:\n",
    "\n",
    "$$ var(x_1, x_2, x_3, \\dots, x_N) = \\frac{1}{N-1} \\sum_{j=1}^N (x_j - \\bar{x})^2  $$\n",
    "\n",
    "and related to that the standard deviation:\n",
    "\n",
    "$$ \\sigma(x_1, x_2, x_3, \\dots, x_N) = \\sqrt{var(x_1, x_2, x_3, \\dots, x_N)} $$\n",
    "\n",
    "Note that the denominator $N-1$ makes this the *unbiased* variance. A denominator of $N$ would be the *biased* variance. The distinction doesn't matter here, but SciPy's stats module uses the biased variance by default.\n",
    "\n",
    "###  Skew\n",
    "\n",
    "The $3^{rd}$ moment indicates the asymmetry of the distribution in\n",
    "terms of a tilt:\n",
    "\n",
    "$$ skew(x_1, x_2, x_3, \\dots, x_N) = \\frac{1}{N} \\sum_{j=1}^N\n",
    "\\left(\\frac{x_j - \\bar{x}}{\\sigma}\\right)^3 $$\n",
    "\n",
    "There is also a distinction between *biased* and *unbia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard normal distribution\n",
    "\n",
    "1. Create data set with a normal distribution.\n",
    "2. Plot a histogram of the distribution.\n",
    "3. Overplot the properly scaled propability density of the normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab nbagg\n",
    "ifig=1;close(ifig);figure(ifig)\n",
    "a=random.standard_normal((2,500))\n",
    "plot(a[0],a[1],'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ifig=2;close(ifig);figure(ifig)\n",
    "#ifig=2;figure(ifig)\n",
    "n=10\n",
    "ah=hist(a.flatten(),n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot distribution\n",
    "y=ah[0]\n",
    "x=ah[1][0:-1]+0.5*diff(ah[1])\n",
    "plot(x,y,'o--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability density of the normal distribution\n",
    "$$ f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp -\\frac{(x-\\mu)^2}{2\\sigma^2})$$\n",
    "where $\\mu$ is the mean, $\\sigma$ is the standard deviation and $\\sigma^2$ is the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pdf_normal(x,mu,var):\n",
    "    thing = sqrt(2*pi*var)\n",
    "    thang = -(x-mu)**2/(2.*var)\n",
    "    return exp(thang)/thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = linspace(-3,3,100)\n",
    "plot(xx,pdf_normal(xx,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize data set\n",
    "\n",
    "Integral of probability density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "integrate.cumtrapz(pdf_normal(x,0,1),x)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of discretized distribution: $$\\frac{(x_\\mathrm{max}-x_\\mathrm{in}) N}{n}$$ where $N$ is the number of points and $n$ is the number of bars in the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=ah[0]/((len(a.flatten())*6/n))\n",
    "x=ah[1][0:-1]+0.5*diff(ah[1])\n",
    "plot(x,y,'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, make a clean plot of PDF (probability density function) and normalized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ifig=3;close(ifig);figure(ifig)\n",
    "plot(x,y,'o')\n",
    "plot(xx,pdf_normal(xx,0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to calculate mean and std deviation\n",
    "\n",
    "1.  Write a function that will take an array of data values and calculates the mean and standard deviation. Add appropriate documentation and make sure that common user errors are escaped.\n",
    "2. Lab: Extend the function with the optional capability to calculate the\n",
    "  variance, standard deviation or skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_mean_var(x):\n",
    "    N = len(x)\n",
    "    m = sum(x)/N\n",
    "    var = sum((x-m)**2)/(N-1)\n",
    "    return m,var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_mean_var(a[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear correlation\n",
    "\n",
    "**Literature:** Press etal. _Numerical Recipies_, Chapter 14.5\n",
    "\n",
    "If we know that two data sets are correlated than the linear correlation coefficient provides a measure of how well they are correlated. This is a different question than _Are the two data sets correlated?_ - and a slightly modified approach is needed to answer that question; see discussion Chapter 14.5, Numerical Recipes.\n",
    "\n",
    "For $N$ pairs of values $(x_i,y_i)$ with $i=1 \\dots N$ this coefficient (also called the product-moment correlation coefficient, or _Pearson's r_) is given by\n",
    "\n",
    "$$ r = \\frac{\\sum_i (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_i (x_i-\\bar{x})^2}\\sqrt{\\sum_i (y_i-\\bar{y})^2}}$$\n",
    "\n",
    "\n",
    "\n",
    "Lab: write a function that provides the _Pearson's r_ coefficient for a correlated set of data pairs.\n",
    "\n",
    "### Fitting data with a model \n",
    "\n",
    "**Literature:** Press etal. _Numerical Recipies_, Chapter 15.1 - 15.2\n",
    "\n",
    "#### $\\chi$-square as merit function\n",
    "Often we have some experimental data, and we have a physics based\n",
    "model in the form of a simple equation that we expect to reproduce\n",
    "trends observed in the data. The model contains undetermined\n",
    "parameters, typically representing unresolved sub-grid physics.\n",
    "\n",
    "Again, we have $N$ pairs of values $(x_i,y_i)$ with $i=1\n",
    "\\dots N$, and we have a model in the form\n",
    "$$ y(x) = y(x; a_1 \\dots a_M)$$\n",
    "\n",
    "In order to determine the parameters $(a_1 \\dots a_M)$ that\n",
    "provide the _maximum likelyhood_ for the data to be a representation\n",
    "of the model (specified by the parameter values $a_j$) we need to\n",
    "adopt a _merit function_ that is arranged so that a minimum in the\n",
    "merit function yields the _best-fit parameters_.\n",
    "\n",
    "Such a _merit function_ may be based on the least-squares fit:\n",
    "\n",
    "mimimize over the parameter values $a_j$:\n",
    "$$ \\sum_{i=1}^N [y_i -  y(x; a_1 \\dots a_M) ]^2$$\n",
    "\n",
    "However, this does not yet include the statistical error on each\n",
    "$y_i$. Therefore we minimize instead over\n",
    "$$\\chi^2 =  \\sum_{i=1}^N \\left( \\frac{y_i -  y(x; a_1 \\dots a_M) }{\\sigma_i}\\right )^2 $$\n",
    "where $\\sigma_i$ is the standard deviation of the point $(x_i,y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting a straight line\n",
    "\n",
    "The most simple model would involve a linear correlation\n",
    "$$ y(x) = y(x; a, b) = a + bx$$\n",
    "\n",
    "\n",
    "\n",
    "##### What is $\\chi^2$ for this model?\n",
    "$$\n",
    "\\chi^2(a,b) = \\sum_{i=1}^N \\left ( \\frac{y_i - a - bx_i}{\\sigma_i} \\right )^2\n",
    "$$\n",
    "\n",
    "##### What are the parameters $a$ and $b$?\n",
    "Derive a condition for $a$ and $b$ considering that $\\chi^2$ has a minimum with respect to $a$ and $b$ when $\\frac{\\partial \\chi^2}{\\partial a} = 0$ and $\\frac{\\partial \\chi^2}{\\partial b} = 0$\n",
    "\n",
    "With \n",
    "$$ S \\equiv \\sum_{i=1}^N \\frac{1}{\\sigma_i^2} $$\n",
    "$$ S_x \\equiv \\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2}\\mathrm{,\\ } S_y \\equiv \\sum_{i=1}^N \\frac{y_i}{\\sigma_i^2} $$\n",
    "$$ S_{xx} \\equiv \\sum_{i=1}^N \\frac{x_i^2}{\\sigma_i^2}\\mathrm{,\\ } S_{xy} \\equiv \\sum_{i=1}^N \\frac{x_i y_i}{\\sigma_i^2} $$\n",
    "\n",
    "we get\n",
    "\n",
    "$$ aS +bS_x = S_y$$\n",
    "$$ aS_x + b S_{xx} = S_{xy}$$\n",
    "\n",
    "with the solution \n",
    "\n",
    "$$\\Delta  \\equiv SS_{xx} - (S_x)^2$$\n",
    "\n",
    "$$a = \\frac{S_{xx}S_y-S_xS_{xy}}{\\Delta}$$\n",
    "\n",
    "$$b = \\frac{SS_{xy}S_y-S_xS_{y}}{\\Delta}$$\n",
    "\n",
    "\n",
    "Exercise for your own review: Create a function that implements $ \\chi^2 $ fitting of a linear relationship to a correlated set of data pairs.\n",
    "\n",
    "\n",
    "This is only the first step. We would also need to determine the _goodness-of-fit_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-square fitting of arbitrary curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=linspace(0,5,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ifig=5;close(ifig);figure(ifig)\n",
    "def func_explore(x):\n",
    "    return exp(-x)\n",
    "#    return x\n",
    "plot(x,func_explore(x),'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_level=0.2\n",
    "noisy_x=x+noise_level*randn(len(x))\n",
    "noisy_y=func_explore(x)+noise_level*randn(len(x))\n",
    "plot(noisy_x,noisy_y,'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ifig=6;close(ifig);figure(ifig)\n",
    "hist(func_explore(x),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "# curve_fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_func(x,a,b,c):\n",
    "    return a*x**3+b*x**2+c\n",
    "#    return a*exp(b*x)+c\n",
    "fitpars, covmat = curve_fit(model_func,noisy_x,noisy_y,p0=[1.5,-1.5,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variances = covmat.diagonal()\n",
    "std_devs = np.sqrt(variances)\n",
    "print(fitpars,std_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ifig=7;close(ifig);figure(ifig)\n",
    "\n",
    "\n",
    "plot(x,func_explore(x),label='org function')    \n",
    "plot(noisy_x,noisy_y,'o',label='noised data')   \n",
    "plot(x,model_func(x,fitpars[0],fitpars[1],fitpars[2]),\\\n",
    "     label='fitted function') # fitted model function\n",
    "legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative libraries\n",
    "\n",
    "There is another fitting method called `polyfit` in the `numpy` package. It is specifically designed to fit data with a power law. Another option is `numpy.linalg.lstsq`. You will have to try them, read the documentation of what algorithms they use, what limitations are specified and try to assess which methods best serves your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## General steps involved in a computational physics problem\n",
    "From this example we can identify the following steps involved in solving a computational physics problem:\n",
    "\n",
    "1. Identify clearly the question.\n",
    "   - this may sound like a trivial step, but in practise a lot of confusion later on can be avoided if the question or problem to be addressed is very clearly defined\n",
    "2. Identify the relevant and important physics that needs to be considered. \n",
    "    - if the problem does not have radiation or magnetic fields, then don't include them; again this sounds trivial but still ...\n",
    "3. Formulate the physics in an appropriate mathematical framework.\n",
    "4. Chose a suitable difference scheme to represent the mathematical equation.\n",
    "    - this step determines the numerical accuracy of the scheme\n",
    "    - it says how well do the difference equations represent the mathematical equations\n",
    "5. Chose the right solution scheme for the difference equation.\n",
    "    - this will effect the stability of the numerical solution\n",
    "    - it will determine how precisely the adopted solution represents the difference equation\n",
    "\n",
    "## Verification and validation\n",
    "A very important final step that often takes significant amount of time and effort is to _check if we get the right answer for the right reasons_. We need to do answer to fundamental questions:\n",
    "\n",
    "1. Have I solved the right equations? This test is called **validation**.\n",
    "2. Have I solved the equations right? This is called **verification**.\n",
    "\n",
    "An important verification test is a numerical convergence study as well as comparison against analytical solutions. The only real validation test is the comparison with experimental data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
